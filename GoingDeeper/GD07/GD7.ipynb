{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "596114d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "2.0.9\n",
      "2.2.1\n",
      "1.21.4\n",
      "1.3.3\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(json.__version__)\n",
    "print(re.__version__)\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c39620",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_32000.model\")\n",
    "\n",
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "    if not vocab.is_unknown(id):\n",
    "        vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddbf6d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지미 카터\n",
      "제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.\n",
      "지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.\n",
      "1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\n",
      "1976년 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워, 포드를 누르고 당선되었다.\n",
      "카터 대통령은 에너지 개발을 촉구했으나 공화당의 반대로 무산되었다.\n",
      "카터는 이집트와 이스라엘을 조정하여, 캠프 데이비드에서 안와르 사다트 대통령과 메나헴 베긴 수상과 함께 중동 평화를 위한 캠프데이비드 협정을 체결했다.\n",
      "그러나 이것은 공화당과 미국의 유대인 단체의 반발을 일으켰다. 1979년 백악관에서 양국 간의 평화조약으로 이끌어졌다. 또한 소련과 제2차 전략 무기 제한 협상에 조인했다.\n",
      "카터는 1970년대 후반 당시 대한민국 등 인권 후진국의 국민들의 인권을 지키기 위해 노력했으며, 취임 이후 계속해서 도덕정치를 내세웠다.\n",
      "그러나 주 이란 미국 대사관 인질 사건에서 인질 구출 실패를 이유로 1980년 대통령 선거에서 공화당의 로널드 레이건 후보에게 져 결국 재선에 실패했다. 또한 임기 말기에 터진 소련의 아프가니스탄 침공 사건으로 인해 1980년 하계 올림픽에 반공국가들의 보이콧을 내세웠다.\n",
      "지미 카터는 대한민국과의 관계에서도 중요한 영향을 미쳤던 대통령 중 하나다. 인권 문제와 주한미군 철수 문제로 한때 한미 관계가 불편하기도 했다. 1978년 대한민국에 대한 북한의 위협에 대비해 한미연합사를 창설하면서, 1982년까지 3단계에 걸쳐 주한미군을 철수하기로 했다. 그러나 주한미군사령부와 정보기관·의회의 반대에 부딪혀 주한미군은 완전철수 대신 6,000명을 감축하는 데 그쳤다. 또한 박정희 정권의 인권 문제 등과의 논란으로 불협화음을 냈으나, 1979년 6월 하순, 대한민국을 방문하여 관계가 다소 회복되었다.\n",
      "1979년 ~ 1980년 대한민국의 정치적 격변기 당시의 대통령이었던 그는 이에 대해 애매한 태도를 보였고, 이는 후에 대한민국 내에서 고조되는 반미 운동의 한 원인이 됐다. 10월 26일, 박정희 대통령이 김재규 중앙정보부장에 의해 살해된 것에 대해 그는 이 사건으로 큰 충격을 받았으며, 사이러스 밴스 국무장관을 조문사절로 파견했다. 12·12 군사 반란과 5.17 쿠데타에 대해 초기에는 강하게 비난했으나, 미국 정부가 신군부를 설득하는데, 한계가 있었고 결국 묵인하는 듯한 태도를 보이게 됐다.\n",
      "퇴임 이후 민간 자원을 적극 활용한 비영리 기구인 카터 재단을 설립한 뒤 민주주의 실현을 위해 제 3세계의 선거 감시 활동 및 기니 벌레에 의한 드라쿤쿠르스 질병 방재를 위해 힘썼다. 미국의 빈곤층 지원 활동, 사랑의 집짓기 운동, 국제 분쟁 중재 등의 활동도 했다.\n",
      "카터는 카터 행정부 이후 미국이 북핵 위기, 코소보 전쟁, 이라크 전쟁과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을 선행하는 행위에 대해 깊은 유감을 표시 하며 미국의 군사적 활동에 강한 반대 입장을 보이고 있다.\n",
      "특히 국제 분쟁 조정을 위해 북한의 김일성, 아이티의 세드라스 장군, 팔레인스타인의 하마스, 보스니아의 세르비아계 정권 같이 미국 정부에 대해 협상을 거부하면서 사태의 위기를 초래한 인물 및 단체를 직접 만나 분쟁의 원인을 근본적으로 해결하기 위해 힘썼다. 이 과정에서 미국 행정부와 갈등을 보이기도 했지만, 전직 대통령의 권한과 재야 유명 인사들의 활약으로 해결해 나갔다.\n",
      "1978년에 채결된 캠프데이비드 협정의 이행이 지지부진 하자 중동 분쟁 분제를 해결하기 위해 1993년 퇴임 후 직접 이스라엘과 팔레인스타인의 오슬로 협정을 이끌어 내는 데도 성공했다.\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(data_dir, 'kowiki.txt')\n",
    "\n",
    "# 파일 열기\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            print(line.strip())  # 한 줄씩 출력\n",
    "            if i >= 15:  # 처음 10줄만 출력\n",
    "                break\n",
    "except FileNotFoundError:\n",
    "    print(f\"파일을 찾을 수 없습니다: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"파일을 여는 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f933315f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# [CLS], tokens a, [SEP], tokens b, [SEP] 형태의 token 생성\n",
    "# CLS : 요약 , SEP : 문장 경계\n",
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30573550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 token의 15% mask\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7bf69d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할 \n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "\n",
    "    # random mask를 위해서 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    # tokens를 복사하여 변경할 준비\n",
    "    masked_tokens = copy.deepcopy(tokens)\n",
    "    mask_lms = []\n",
    "\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 현재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "        for index in index_set:\n",
    "            if len(mask_lms) >= mask_cnt:\n",
    "                break\n",
    "\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9:  # 10% keep original\n",
    "                masked_token = masked_tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            masked_tokens[index] = masked_token\n",
    "\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "    return masked_tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "668a7c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '[MASK]', '▁삼', '십', '▁전', '▁둘째', '번', '[MASK]', '[MASK]', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁그레이트', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘러', '[MASK]', '[MASK]', '[MASK]', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [18, 24, 25, 31, 44, 45, 49, 50, 51, 63]\n",
      "mask_label : ['▁번에', '▁오', '십', '▁십', '▁기쁨', '의', '▁컬', '컬', '한', '▁전부터']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e186332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.\n",
    "지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.\n",
    "1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\n",
    "1976년 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워, 포드를 누르고 당선되었다.\n",
    "카터 대통령은 에너지 개발을 촉구했으나 공화당의 반대로 무산되었다.\n",
    "카터는 이집트와 이스라엘을 조정하여, 캠프 데이비드에서 안와르 사다트 대통령과 메나헴 베긴 수상과 함께 중동 평화를 위한 캠프데이비드 협정을 체결했다.\n",
    "그러나 이것은 공화당과 미국의 유대인 단체의 반발을 일으켰다. 1979년 백악관에서 양국 간의 평화조약으로 이끌어졌다. 또한 소련과 제2차 전략 무기 제한 협상에 조인했다.\n",
    "카터는 1970년대 후반 당시 대한민국 등 인권 후진국의 국민들의 인권을 지키기 위해 노력했으며, 취임 이후 계속해서 도덕정치를 내세웠다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13710c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카터', '▁주니어', '(,', '▁1924', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주당', '▁출신', '▁미국', '▁39', '번째', '▁대통령', '▁(19', '77', '년', '▁~', '▁1981', '년', ')', '이다', '.']\n",
      "['▁지미', '▁카', '터는', '▁조지아', '주', '▁섬', '터', '▁카운티', '▁플', '레인', '스', '▁마을에서', '▁태어났다', '.', '▁조지아', '▁공과', '대학교를', '▁졸업하였다', '.', '▁그', '▁후', '▁해군에', '▁들어가', '▁전함', '·', '원자', '력', '·', '잠', '수', '함의', '▁승무', '원으로', '▁일하였다', '.', '▁1953', '년', '▁미국', '▁해군', '▁대', '위로', '▁예편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '▁등을', '▁가', '꿔', '▁많은', '▁돈을', '▁벌', '었다', '.', '▁그의', '▁별명이', '▁\"', '땅', '콩', '▁농부', '\"', '▁(', 'P', 'ean', 'ut', '▁F', 'ar', 'mer', ')', '로', '▁알려졌다', '.']\n",
      "['▁1962', '년', '▁조지아', '▁주', '▁상원', '▁의원', '▁선거에서', '▁낙선', '하나', '▁그', '▁선거가', '▁부정', '선거', '▁', '였', '음을', '▁입증', '하게', '▁되어', '▁당선', '되고', ',', '▁1966', '년', '▁조지아', '▁주', '▁지사', '▁선거에', '▁낙선', '하지만', '▁1970', '년', '▁조지아', '▁주', '▁지', '사를', '▁역임했다', '.', '▁대통령이', '▁되기', '▁전', '▁조지아', '주', '▁상원의', '원을', '▁두', '번', '▁연', '임', '했으며', ',', '▁1971', '년부터', '▁1975', '년까지', '▁조지아', '▁지', '사로', '▁근무했다', '.', '▁조지아', '▁주지', '사로', '▁지내', '면서', ',', '▁미국에', '▁사는', '▁흑인', '▁등용', '법을', '▁내세', '웠다', '.']\n",
      "['▁1976', '년', '▁대통령', '▁선거에', '▁민주당', '▁후보로', '▁출마하여', '▁도덕', '주의', '▁정책으로', '▁내세워', ',', '▁포', '드를', '▁누르고', '▁당선되었다', '.']\n",
      "['▁카터', '▁대통령은', '▁에너지', '▁개발을', '▁촉구', '했으나', '▁공화', '당의', '▁반대로', '▁무산되었다', '.']\n",
      "['▁카', '터는', '▁이집', '트와', '▁이스라엘', '을', '▁조정', '하여', ',', '▁캠프', '▁데이비', '드에서', '▁안', '와', '르', '▁사다', '트', '▁대통령과', '▁메', '나', '헴', '▁베', '긴', '▁수상', '과', '▁함께', '▁중동', '▁평화를', '▁위한', '▁캠프', '데이', '비', '드', '▁협정을', '▁체결했다', '.']\n",
      "['▁그러나', '▁이것은', '▁공화', '당과', '▁미국의', '▁유대인', '▁단체의', '▁반발을', '▁일으켰다', '.', '▁1979', '년', '▁백악', '관에서', '▁양국', '▁간의', '▁평화', '조약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련과', '▁제', '2', '차', '▁전략', '▁무기', '▁제한', '▁협', '상에', '▁조인', '했다', '.']\n",
      "['▁카', '터는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인권', '▁후진', '국의', '▁국민들의', '▁인', '권을', '▁지키기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속해서', '▁도덕', '정', '치를', '▁내세', '웠다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "for tokens in doc:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b87c6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94cacca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2baf5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    :param vocab: SentencePieceProcessor\n",
    "    :param doc: 줄 단위로 토큰화된 문서\n",
    "    :param n_seq: 최대 시퀀스 길이\n",
    "    :param mask_prob: 마스킹 확률\n",
    "    :param vocab_list: 랜덤 토큰 생성용 vocab 리스트\n",
    "    :return: pretrain 인스턴스 리스트\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3  \n",
    "\n",
    "    instances = []\n",
    "    current_chunk = [] # line 단위 tokens\n",
    "    current_length = 0\n",
    "\n",
    "    for i in range(len(doc)): # doc 전체를 loop\n",
    "        current_chunk.append(doc[i])  # line 단위로 추가\n",
    "        current_length += len(doc[i])  # # current_chunk의 token 수\n",
    "\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄이거나 길이가 max_seq 이상인 경우\n",
    "            \n",
    "            # token a \n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "\n",
    "            # token b \n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0    # False\n",
    "                tokens_a, tokens_b = tokens_b, tokens_a\n",
    "            else:\n",
    "                is_next = 1    # True\n",
    "\n",
    "            # max_seq보다 큰 경우 길이 조절\n",
    "            while len(tokens_a) + len(tokens_b) > max_seq:\n",
    "                if len(tokens_a) > len(tokens_b):\n",
    "                    tokens_a.pop(0)\n",
    "                else:\n",
    "                    tokens_b.pop()\n",
    "\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "\n",
    "            # tokens & segment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd583c0",
   "metadata": {},
   "source": [
    "### seq1,seq2,seq3 -> seq1,seq2과 seq2,seq3 처럼 슬라이딩 방식으로 오게 해야 더 성능이 좋지 않을까?\n",
    "    * MASK가 유출될 가능성 때문에 안 될 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f21d7c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁\"', '지', '미', '\"', '▁카터', '▁주니어', '(,', '▁1924', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주당', '[MASK]', '▁미국', '▁주어진', '▁Sw', '▁대통령', '▁(19', '77', '년', '[MASK]', '▁1981', '년', ')', '이다', '.', '[SEP]', '[MASK]', '▁카', '터는', '▁조지아', '주', '▁섬', '터', '▁카운티', '▁플', '레인', '스', '▁마을에서', '▁태어났다', '.', '▁조지아', '▁공과', '대학교를', '▁졸업하였다', '.', '▁그', '▁후', '[MASK]', '▁들어가', '▁전함', '·', '원자', '력', '·', '잠', '수', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [18, 20, 21, 23, 24, 25, 26, 33, 54], 'mask_label': ['▁출신', '▁39', '번째', '▁(19', '77', '년', '▁~', '▁지미', '▁해군에']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁조지아', '▁주', '▁지', '사를', '▁역임했다', '.', '▁대통령이', '▁되기', '[MASK]', '▁조지아', '주', '▁상원의', '원을', '▁두', '번', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁1971', '년부터', '▁1975', '년까지', '▁조지아', '▁지', '사로', '▁근무했다', '.', '▁조지아', '▁주지', '사로', '▁지내', '면서', ',', '▁미국에', '▁사는', '▁흑인', '▁등용', '법을', '▁내세', '웠다', '.', '[SEP]', '[MASK]', '[MASK]', '▁대통령', '▁선거에', '▁민주당', '▁후보로', '▁출마하여', '▁도덕', '주의', '▁정책으로', '▁내세워', ',', '▁포', '드를', '▁누르고', '▁당선되었다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 11, 18, 19, 20, 21, 46, 47], 'mask_label': ['▁1970', '년', '▁전', '▁연', '임', '했으며', ',', '▁1976', '년']}\n",
      "{'tokens': ['[CLS]', '▁카터', '가', '▁에너지', '▁개발을', '▁촉구', '했으나', '▁공화', '당의', '▁반대로', '▁무산되었다', '.', '[SEP]', '▁카', '터는', '▁이집', '트와', '[MASK]', '[MASK]', '▁조정', '하여', ',', '▁캠프', '▁데이비', '드에서', '▁안', '와', '르', '[MASK]', '[MASK]', '▁대통령과', '[MASK]', '[MASK]', '[MASK]', '▁베', '긴', '▁수상', '과', '▁함께', '▁중동', '▁평화를', '▁위한', '▁캠프', '데이', '비', '드', '▁협정을', '▁체결했다', '.', '▁그러나', '▁이것은', '▁공화', '당과', '▁미국의', '▁유대인', '▁단체의', '[MASK]', '▁일으켰다', '.', '▁1979', '년', '▁백악', '관에서', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [2, 17, 18, 28, 29, 31, 32, 33, 56], 'mask_label': ['▁대통령은', '▁이스라엘', '을', '▁사다', '트', '▁메', '나', '헴', '▁반발을']}\n"
     ]
    }
   ],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb47679a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b98dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)  \n",
    "            if 0 < len(doc):\n",
    "                if 0 < count:\n",
    "                    count -= 1\n",
    "                    print(len(doc), \"lines :\", doc[0])\n",
    "                    print(doc[1])\n",
    "                    print(doc[-1])\n",
    "                    print()\n",
    "                else:\n",
    "                    break\n",
    "                doc = []\n",
    "        else:  # 빈 줄이 아니면 doc에 저장\n",
    "            pieces = vocab.encode_as_pieces(line)    \n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        print(doc[0])\n",
    "        print(doc[1])\n",
    "        print(doc[-1])\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60b21dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb29b49d5db4b20bad927fd90c52e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 21 instances: 10\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '[MASK]', '[MASK]', '[MASK]', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 5, 22, 23, 24, 41, 61, 62], 'mask_label': ['▁X', '선', '▁등이', '▁고분', '자', '물질', '▁연구하는', '▁지금은', '▁유기']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '[MASK]', '[MASK]', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '[MASK]', '[MASK]', '[MASK]', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '[MASK]', '▁화합', '물을', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '[MASK]', '[MASK]', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [9, 10, 22, 23, 24, 38, 41, 52, 53], 'mask_label': ['▁분석', '에', '▁고분', '자', '물질', '▁이루어진', '▁연구하는', '▁동물', '로부터']}\n",
      "\n",
      "doc: 14 instances: 7\n",
      "{'tokens': ['[CLS]', '▁X', '선', '[MASK]', '[MASK]', '▁기기', '▁개발되어', '[MASK]', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '[MASK]', '[MASK]', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '[MASK]', '[MASK]', '[MASK]', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [3, 4, 5, 7, 39, 40, 54, 55, 56], 'mask_label': ['▁결정', '학', '▁등이', '▁유기', '▁화합', '물을', '▁추출', '해', '낸']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '[MASK]', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '[MASK]', '▁매우', '▁중요한', '▁방법으로', '[MASK]', '[MASK]', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁남편', '주의가', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [5, 8, 11, 15, 16, 52, 53, 61, 62], 'mask_label': ['▁등이', '▁화합물', '▁있어서', '▁자리잡았다', '.', '▁동물', '로부터', '▁지금은', '▁유기']}\n",
      "\n",
      "doc: 4 instances: 2\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '[MASK]', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '[MASK]', '▁유기', '화', '학에서', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '▁화합', '물은', '▁식물', '이나', '[MASK]', '[MASK]', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [5, 25, 29, 30, 31, 47, 52, 53, 62], 'mask_label': ['▁등이', '▁등도', '▁다루', '어진다', '.', '▁유기', '▁동물', '로부터', '▁유기']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '[MASK]', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁검찰', '▁IP', '▁합성', '섬유', '등의', '[MASK]', '[MASK]', '[MASK]', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '[MASK]', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [12, 17, 18, 22, 23, 24, 38, 59, 60], 'mask_label': ['▁매우', '▁플라스틱', ',', '▁고분', '자', '물질', '▁이루어진', '▁뜻', '하였으나']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '[MASK]', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '[MASK]', '[MASK]', '[MASK]', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '[MASK]', '[MASK]', '[MASK]', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [6, 19, 20, 21, 48, 49, 54, 55, 56], 'mask_label': ['▁개발되어', '▁합성', '섬유', '등의', '▁화합', '물은', '▁추출', '해', '낸']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '[MASK]', '[MASK]', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [7, 8, 39, 40, 52, 53, 59, 60, 62], 'mask_label': ['▁유기', '▁화합물', '▁화합', '물을', '▁동물', '로부터', '▁뜻', '하였으나', '▁유기']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '[MASK]', '▁자리잡았다', '.', '[MASK]', '[MASK]', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '[MASK]', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁세로', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [14, 17, 18, 33, 34, 35, 46, 61, 62], 'mask_label': ['▁방법으로', '▁플라스틱', ',', '▁유기', '화', '학은', '▁원래', '▁지금은', '▁유기']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '[MASK]', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '[MASK]', '[MASK]', '[MASK]', '▁고분', '자', '물질', '[MASK]', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '[MASK]', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [13, 19, 20, 21, 22, 23, 24, 25, 61], 'mask_label': ['▁중요한', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁지금은']}\n",
      "\n",
      "doc: 31 instances: 15\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '[MASK]', '[MASK]', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 9, 10, 11, 12, 47, 57, 58], 'mask_label': ['▁X', '선', '▁분석', '에', '▁있어서', '▁매우', '▁유기', '▁화합', '물을']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '[MASK]', '▁유기', '[MASK]', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '[MASK]', '[MASK]', '[MASK]', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '▁지금은', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [6, 8, 22, 23, 24, 47, 59, 60, 62], 'mask_label': ['▁개발되어', '▁화합물', '▁고분', '자', '물질', '▁유기', '▁뜻', '하였으나', '▁유기']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instance 생성 기능 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "            if 0 < len(doc):\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                # save\n",
    "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "                print(instances[0])\n",
    "                print(instances[-1])\n",
    "                print()\n",
    "                doc = []\n",
    "                if 0 < count:  # 테스트를 위해서 부분 처리함\n",
    "                    count -= 1\n",
    "                else:\n",
    "                    break\n",
    "        else:  # doc에 저장\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        instances = create_pretrain_instances(doc, 128)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f592dc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9a6f38654145e9ab0ffb155b4113c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_lengths = []  # 시퀀스 길이를 저장할 리스트\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # 빈 줄로 새로운 단락 구분\n",
    "            if 0 < len(doc):\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                # 각 instance의 토큰 길이를 저장\n",
    "                seq_lengths.extend([len(instance['tokens']) for instance in instances])\n",
    "                doc = []\n",
    "        else:  # 빈 줄이 아닌 경우\n",
    "            pieces = vocab.encode_as_pieces(line)  # 토크나이즈\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막 남은 문단 처리\n",
    "        instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "        seq_lengths.extend([len(instance['tokens']) for instance in instances])\n",
    "\n",
    "# 시퀀스 길이 분포 시각화\n",
    "plt.hist(seq_lengths, bins=20, edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Token Sequence Length Distribution\")\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd85c87",
   "metadata": {},
   "source": [
    "* 두 개의 sequence를 이어내야하니까 토큰 128은 적절한 값으로 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e004f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    # instances\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count \n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4230520b",
   "metadata": {},
   "source": [
    "* 문단 단위로 인스턴스 생성 \n",
    "* 두 문장을 SEP로 이어붙임 400만개 -> 200만개\n",
    "* 한 문장만 존재하는 문단은 탈락 & 128 token을 넘어가는 문단 탈락 200만개 -> 86만개 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1964f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data: 862285\n",
      "Instance 1:\n",
      "{\n",
      "    \"tokens\": [\n",
      "        \"[CLS]\",\n",
      "        \"▁태어났다\",\n",
      "        \".\",\n",
      "        \"▁조지아\",\n",
      "        \"▁공과\",\n",
      "        \"대학교를\",\n",
      "        \"▁졸업하였다\",\n",
      "        \".\",\n",
      "        \"▁그\",\n",
      "        \"[MASK]\",\n",
      "        \"▁해군에\",\n",
      "        \"▁들어가\",\n",
      "        \"▁전함\",\n",
      "        \"·\",\n",
      "        \"원자\",\n",
      "        \"력\",\n",
      "        \"·\",\n",
      "        \"잠\",\n",
      "        \"수\",\n",
      "        \"함의\",\n",
      "        \"▁승무\",\n",
      "        \"원으로\",\n",
      "        \"▁일하였다\",\n",
      "        \".\",\n",
      "        \"▁1953\",\n",
      "        \"년\",\n",
      "        \"▁미국\",\n",
      "        \"▁해군\",\n",
      "        \"▁대\",\n",
      "        \"위로\",\n",
      "        \"▁예편\",\n",
      "        \"하였고\",\n",
      "        \"▁이후\",\n",
      "        \"▁땅\",\n",
      "        \"콩\",\n",
      "        \"·\",\n",
      "        \"면\",\n",
      "        \"화\",\n",
      "        \"▁등을\",\n",
      "        \"▁가\",\n",
      "        \"꿔\",\n",
      "        \"▁많은\",\n",
      "        \"▁돈을\",\n",
      "        \"▁벌\",\n",
      "        \"었다\",\n",
      "        \".\",\n",
      "        \"▁그의\",\n",
      "        \"▁별명이\",\n",
      "        \"[MASK]\",\n",
      "        \"[MASK]\",\n",
      "        \"[MASK]\",\n",
      "        \"▁농부\",\n",
      "        \"\\\"\",\n",
      "        \"▁(\",\n",
      "        \"P\",\n",
      "        \"ean\",\n",
      "        \"ut\",\n",
      "        \"[MASK]\",\n",
      "        \"[MASK]\",\n",
      "        \"[MASK]\",\n",
      "        \"[MASK]\",\n",
      "        \"[MASK]\",\n",
      "        \"▁알려졌다\",\n",
      "        \".\",\n",
      "        \"[SEP]\",\n",
      "        \"▁늦\",\n",
      "        \"되면서\",\n",
      "        \"[MASK]\",\n",
      "        \"▁주\",\n",
      "        \"▁상원\",\n",
      "        \"▁의원\",\n",
      "        \"▁선거에서\",\n",
      "        \"▁낙선\",\n",
      "        \"하나\",\n",
      "        \"▁그\",\n",
      "        \"▁선거가\",\n",
      "        \"▁부정\",\n",
      "        \"선거\",\n",
      "        \"▁\",\n",
      "        \"였\",\n",
      "        \"음을\",\n",
      "        \"▁입증\",\n",
      "        \"하게\",\n",
      "        \"▁되어\",\n",
      "        \"▁당선\",\n",
      "        \"되고\",\n",
      "        \",\",\n",
      "        \"▁1966\",\n",
      "        \"년\",\n",
      "        \"▁조지아\",\n",
      "        \"▁주\",\n",
      "        \"▁지사\",\n",
      "        \"▁선거에\",\n",
      "        \"▁낙선\",\n",
      "        \"하지만\",\n",
      "        \"[MASK]\",\n",
      "        \"[MASK]\",\n",
      "        \"▁조지아\",\n",
      "        \"▁주\",\n",
      "        \"▁지\",\n",
      "        \"사를\",\n",
      "        \"▁역임했다\",\n",
      "        \".\",\n",
      "        \"[MASK]\",\n",
      "        \"▁되기\",\n",
      "        \"▁전\",\n",
      "        \"▁조지아\",\n",
      "        \"주\",\n",
      "        \"▁상원의\",\n",
      "        \"원을\",\n",
      "        \"▁두\",\n",
      "        \"번\",\n",
      "        \"▁연\",\n",
      "        \"임\",\n",
      "        \"했으며\",\n",
      "        \",\",\n",
      "        \"▁1971\",\n",
      "        \"년부터\",\n",
      "        \"[MASK]\",\n",
      "        \"[MASK]\",\n",
      "        \"▁조지아\",\n",
      "        \"▁지\",\n",
      "        \"사로\",\n",
      "        \"▁근무했다\",\n",
      "        \".\",\n",
      "        \"▁조지아\",\n",
      "        \"[MASK]\",\n",
      "        \"[SEP]\"\n",
      "    ],\n",
      "    \"segment\": [\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1\n",
      "    ],\n",
      "    \"is_next\": 1,\n",
      "    \"mask_idx\": [\n",
      "        9,\n",
      "        48,\n",
      "        49,\n",
      "        50,\n",
      "        57,\n",
      "        58,\n",
      "        59,\n",
      "        60,\n",
      "        61,\n",
      "        65,\n",
      "        66,\n",
      "        67,\n",
      "        95,\n",
      "        96,\n",
      "        103,\n",
      "        118,\n",
      "        119,\n",
      "        126\n",
      "    ],\n",
      "    \"mask_label\": [\n",
      "        \"▁후\",\n",
      "        \"▁\\\"\",\n",
      "        \"땅\",\n",
      "        \"콩\",\n",
      "        \"▁F\",\n",
      "        \"ar\",\n",
      "        \"mer\",\n",
      "        \")\",\n",
      "        \"로\",\n",
      "        \"▁1962\",\n",
      "        \"년\",\n",
      "        \"▁조지아\",\n",
      "        \"▁1970\",\n",
      "        \"년\",\n",
      "        \"▁대통령이\",\n",
      "        \"▁1975\",\n",
      "        \"년까지\",\n",
      "        \"▁주지\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "\n",
      "Instance 2:\n",
      "{\n",
      "    \"tokens\": [\n",
      "        \"[CLS]\",\n",
      "        \"▁1976\",\n",
      "        \"년\",\n",
      "        \"▁대통령\",\n",
      "        \"▁선거에\",\n",
      "        \"▁민주당\",\n",
      "        \"▁후보로\",\n",
      "        \"▁출마하여\",\n",
      "        \"▁도덕\",\n",
      "        \"주의\",\n",
      "        \"▁정책으로\",\n",
      "        \"▁내세워\",\n",
      "        \",\",\n",
      "        \"[MASK]\",\n",
      "        \"[MASK]\",\n",
      "        \"▁누르고\",\n",
      "        \"▁당선되었다\",\n",
      "        \".\",\n",
      "        \"▁카터\",\n",
      "        \"▁대통령은\",\n",
      "        \"▁에너지\",\n",
      "        \"▁개발을\",\n",
      "        \"▁촉구\",\n",
      "        \"했으나\",\n",
      "        \"▁공화\",\n",
      "        \"당의\",\n",
      "        \"▁반대로\",\n",
      "        \"▁무산되었다\",\n",
      "        \".\",\n",
      "        \"[SEP]\",\n",
      "        \"▁카\",\n",
      "        \"터는\",\n",
      "        \"▁이집\",\n",
      "        \"트와\",\n",
      "        \"▁이스라엘\",\n",
      "        \"을\",\n",
      "        \"▁조정\",\n",
      "        \"하여\",\n",
      "        \",\",\n",
      "        \"▁캠프\",\n",
      "        \"▁데이비\",\n",
      "        \"드에서\",\n",
      "        \"▁안\",\n",
      "        \"와\",\n",
      "        \"르\",\n",
      "        \"[MASK]\",\n",
      "        \"[MASK]\",\n",
      "        \"▁대통령과\",\n",
      "        \"▁메\",\n",
      "        \"나\",\n",
      "        \"헴\",\n",
      "        \"▁베\",\n",
      "        \"긴\",\n",
      "        \"▁수상\",\n",
      "        \"과\",\n",
      "        \"▁함께\",\n",
      "        \"▁중동\",\n",
      "        \"[MASK]\",\n",
      "        \"▁위한\",\n",
      "        \"▁캠프\",\n",
      "        \"데이\",\n",
      "        \"비\",\n",
      "        \"드\",\n",
      "        \"▁협정을\",\n",
      "        \"▁체결했다\",\n",
      "        \".\",\n",
      "        \"▁그러나\",\n",
      "        \"▁이것은\",\n",
      "        \"▁공화\",\n",
      "        \"당과\",\n",
      "        \"▁미국의\",\n",
      "        \"▁유대인\",\n",
      "        \"[MASK]\",\n",
      "        \"▁반발을\",\n",
      "        \"▁일으켰다\",\n",
      "        \".\",\n",
      "        \"▁1979\",\n",
      "        \"년\",\n",
      "        \"▁백악\",\n",
      "        \"관에서\",\n",
      "        \"▁양국\",\n",
      "        \"▁간의\",\n",
      "        \"▁평화\",\n",
      "        \"조약\",\n",
      "        \"으로\",\n",
      "        \"[MASK]\",\n",
      "        \"[MASK]\",\n",
      "        \"[MASK]\",\n",
      "        \"▁또한\",\n",
      "        \"▁소련과\",\n",
      "        \"▁제\",\n",
      "        \"2\",\n",
      "        \"차\",\n",
      "        \"▁전략\",\n",
      "        \"▁무기\",\n",
      "        \"[MASK]\",\n",
      "        \"▁협\",\n",
      "        \"상에\",\n",
      "        \"▁조인\",\n",
      "        \"했다\",\n",
      "        \".\",\n",
      "        \"▁카\",\n",
      "        \"터는\",\n",
      "        \"[MASK]\",\n",
      "        \"[MASK]\",\n",
      "        \"[MASK]\",\n",
      "        \"▁당시\",\n",
      "        \"[MASK]\",\n",
      "        \"▁등\",\n",
      "        \"▁인권\",\n",
      "        \"▁후진\",\n",
      "        \"국의\",\n",
      "        \"▁국민들의\",\n",
      "        \"▁인\",\n",
      "        \"권을\",\n",
      "        \"▁지키기\",\n",
      "        \"▁위해\",\n",
      "        \"▁노력\",\n",
      "        \"했으며\",\n",
      "        \",\",\n",
      "        \"[MASK]\",\n",
      "        \"▁이후\",\n",
      "        \"▁계속해서\",\n",
      "        \"▁도덕\",\n",
      "        \"정\",\n",
      "        \"치를\",\n",
      "        \"▁내세\",\n",
      "        \"[SEP]\"\n",
      "    ],\n",
      "    \"segment\": [\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        0,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1,\n",
      "        1\n",
      "    ],\n",
      "    \"is_next\": 1,\n",
      "    \"mask_idx\": [\n",
      "        13,\n",
      "        14,\n",
      "        45,\n",
      "        46,\n",
      "        51,\n",
      "        52,\n",
      "        57,\n",
      "        72,\n",
      "        73,\n",
      "        85,\n",
      "        86,\n",
      "        87,\n",
      "        95,\n",
      "        103,\n",
      "        104,\n",
      "        105,\n",
      "        107,\n",
      "        120\n",
      "    ],\n",
      "    \"mask_label\": [\n",
      "        \"▁포\",\n",
      "        \"드를\",\n",
      "        \"▁사다\",\n",
      "        \"트\",\n",
      "        \"▁베\",\n",
      "        \"긴\",\n",
      "        \"▁평화를\",\n",
      "        \"▁단체의\",\n",
      "        \"▁반발을\",\n",
      "        \"▁이끌\",\n",
      "        \"어졌다\",\n",
      "        \".\",\n",
      "        \"▁제한\",\n",
      "        \"▁1970\",\n",
      "        \"년대\",\n",
      "        \"▁후반\",\n",
      "        \"▁대한민국\",\n",
      "        \"▁취임\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VOCAB 32000짜리로 잘 전처리된 corpus json data\n",
    "\n",
    "json_file_path = os.path.join(os.getenv('HOME'), 'aiffel/bert_pretrain/data/bert_pre_train.json')\n",
    "\n",
    "data = []\n",
    "\n",
    "try:\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # \\n 으로 객체 구별\n",
    "            instance = json.loads(line.strip())\n",
    "            data.append(instance)\n",
    "\n",
    "    print(f\"total data: {len(data)}\")\n",
    "    for i, instance in enumerate(data[:2]):  # 첫 2개 출력\n",
    "        print(f\"Instance {i+1}:\")\n",
    "        print(json.dumps(instance, ensure_ascii=False, indent=4))\n",
    "        print(\"\\n\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSONDecodeError: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Another Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac601002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c414421a62432dbbed855136f18e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrain/data/bert_pre_train.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74e93099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "862285"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00456af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 0,\n",
       " 0,\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 128\n",
    "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_seq - 3\n",
    "\n",
    "# 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
    "# enc_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# dec_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# labels_nsp = np.zeros((total,), np.int32)\n",
    "# labels_mlm = np.zeros((total, n_seq), np.int32)\n",
    "\n",
    "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e075216b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45de5389942544e3a502d36c3afde637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/862285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁지미', '[MASK]', '[SEP]', '▁제임스', '▁얼', '▁\"', '지', '미', '\"', '[MASK]', '▁주니어', '(,', '▁1924', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주당', '▁출신', '▁미국', '▁39', '번째', '▁대통령', '▁와인', '▁용어이다', '▁이것이', '▁~', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁지미', '▁카', '터는', '▁조지아', '주', '▁섬', '터', '▁카운티', '▁플', '레인', '스', '▁마을에서', '▁태어났다', '.', '▁조지아', '[MASK]', '[MASK]', '▁졸업하였다', '.', '[MASK]', '▁후', '▁해군에', '[MASK]', '▁전함', '·', '원자', '력', '·', '잠', '수', '함의', '▁승무', '원으로', '▁일하였다', '.', '▁1953', '년', '▁미국', '[MASK]', '▁대', '위로', '▁예편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '▁등을', '▁가', '꿔', '▁많은', '▁돈을', '▁벌', '었다', '.', '▁그의', '▁별명이', '▁\"', '땅', '콩', '▁농부', '\"', '▁(', 'P', 'ean', 'ut', '▁F', 'ar', 'mer', ')', '로', '▁알려졌다', '.', '▁1962', '년', '▁조지아', '▁주', '[MASK]', '▁의원', '▁선거에서', '▁낙선', '하나', '▁그', '▁선거가', '▁부정', '선거', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [2, 10, 28, 29, 30, 32, 33, 34, 35, 36, 52, 53, 56, 59, 75, 116, 125, 126], 'mask_label': ['▁카터', '▁카터', '▁(19', '77', '년', '▁1981', '년', ')', '이다', '.', '▁공과', '대학교를', '▁그', '▁들어가', '▁해군', '▁상원', '▁', '였']}\n",
      "enc_token: [5, 16415, 6, 4, 3324, 1042, 103, 27610, 27686, 27718, 6, 7504, 416, 5708, 27625, 131, 27662, 7, 27629, 203, 241, 27602, 4867, 788, 243, 5898, 796, 663, 17013, 14725, 3396, 203, 6, 6, 6, 6, 6, 16415, 207, 4612, 5551, 27646, 630, 27714, 4269, 429, 5346, 27626, 14406, 1605, 27599, 5551, 6, 6, 8637, 27599, 6, 81, 25987, 6, 15033, 27873, 14475, 27813, 27873, 28196, 27636, 10185, 16285, 1232, 22935, 27599, 4777, 27625, 243, 6, 14, 1509, 22095, 414, 165, 1697, 28290, 27873, 27703, 27683, 593, 21, 29007, 399, 5540, 813, 17, 27599, 307, 16905, 103, 28313, 28290, 19041, 27718, 98, 27878, 15784, 2543, 309, 337, 5771, 27616, 27603, 4578, 27599, 3715, 27625, 5551, 37, 6, 2378, 5249, 9858, 3294, 13, 20590, 2386, 2163, 6, 6, 4]\n",
      "segment: [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0 25250     0     0     0     0     0     0     0 25250     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0  1647  4630 27625     0  3008 27625 27616    16\n",
      " 27599     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0 14146 15991     0     0    13     0     0  2247\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0  2780     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0 11234     0     0     0\n",
      "     0     0     0     0     0 27596 27671     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁카', '터는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인권', '▁후진', '국의', '▁국민들의', '▁인', '권을', '▁지키기', '▁위해', '[MASK]', '[MASK]', '[MASK]', '▁취임', '[MASK]', '▁계속해서', '▁도덕', '정', '치를', '▁내세', '웠다', '.', '[SEP]', '▁1976', '년', '[MASK]', '▁선거에', '▁민주당', '▁후보로', '▁출마하여', '▁도덕', '주의', '▁정책으로', '▁내세워', ',', '▁포', '드를', '[MASK]', '▁당선되었다', '.', '▁카터', '▁대통령은', '▁에너지', '▁개발을', '▁촉구', '했으나', '▁공화', '당의', '▁반대로', '▁무산되었다', '.', '▁카', '터는', '▁이집', '트와', '▁이스라엘', '을', '▁조정', '하여', ',', '▁캠프', '▁데이비', '드에서', '[MASK]', '[MASK]', '[MASK]', '▁사다', '트', '▁대통령과', '▁메', '나', '헴', '[MASK]', '[MASK]', '▁수상', '과', '▁함께', '▁중동', '▁평화를', '▁위한', '▁캠프', '데이', '비', '드', '▁협정을', '▁체결했다', '.', '▁그러나', '▁이것은', '▁공화', '당과', '▁미국의', '▁유대인', '▁단체의', '▁반발을', '[MASK]', '[MASK]', '▁1979', '년', '내고', \"'\", '▁양국', '[MASK]', '▁평화', '조약', '으로', '▁이끌', '어졌다', '.', '▁또한', '[MASK]', '▁제', '2', '차', '▁전략', '▁무기', '[MASK]', '▁협', '상에', '▁조인', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [17, 18, 19, 21, 32, 44, 70, 71, 72, 79, 80, 102, 103, 106, 107, 109, 117, 123], 'mask_label': ['▁노력', '했으며', ',', '▁이후', '▁대통령', '▁누르고', '▁안', '와', '르', '▁베', '긴', '▁일으켰다', '.', '▁백악', '관에서', '▁간의', '▁소련과', '▁제한']}\n",
      "enc_token: [5, 207, 4612, 1921, 596, 1840, 316, 410, 50, 5636, 17092, 137, 18896, 42, 917, 15177, 231, 6, 6, 6, 2659, 6, 6357, 6244, 27642, 1233, 5890, 1853, 27599, 4, 3306, 27625, 6, 8198, 4867, 4896, 19160, 6244, 238, 22033, 19990, 27604, 119, 1486, 6, 7965, 27599, 25250, 5906, 3634, 8085, 9747, 1003, 4460, 1547, 4771, 18474, 27599, 207, 4612, 2703, 3604, 3426, 27607, 3358, 54, 27604, 10251, 3640, 3552, 6, 6, 6, 7025, 27677, 13799, 334, 27637, 29887, 6, 6, 1011, 27644, 280, 8021, 14237, 521, 10251, 4282, 27694, 27681, 15990, 19102, 27599, 330, 1487, 4460, 4040, 679, 7455, 15747, 21408, 6, 6, 2995, 27625, 2655, 27762, 13195, 6, 2793, 8993, 9, 1435, 2521, 27599, 276, 6, 30, 27619, 27751, 2835, 3841, 6, 617, 1824, 15876, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0  3375   530 27604     0   165     0     0\n",
      "     0     0     0     0     0     0     0     0   663     0     0     0\n",
      "     0     0     0     0     0     0     0     0 10071     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0   172 27665\n",
      " 27699     0     0     0     0     0     0   271 28099     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0  6564 27599     0     0 10312  6749\n",
      "     0  2714     0     0     0     0     0     0     0 23197     0     0\n",
      "     0     0     0  1956     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁그러나', '▁주', '▁이란', '[MASK]', '▁대사관', '▁인', '질', '▁사건에서', '▁인', '질', '▁구출', '▁실패를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거에서', '▁공화', '당의', '[MASK]', '[MASK]', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '[MASK]', '[MASK]', '▁실패했다', '.', '▁또한', '▁임기', '[MASK]', '▁터', '진', '[MASK]', '[MASK]', '▁침공', '▁사건으로', '▁인해', '[MASK]', '[MASK]', '[MASK]', '▁올림픽에', '▁반공', '국가', '들의', '▁보이', '콧', '을', '▁내세', '웠다', '.', '[SEP]', '▁지미', '▁카', '터는', '▁대한민국', '과의', '▁관계', '에서도', '▁중요한', '▁영향을', '▁미', '쳤던', '▁대통령', '▁중', '▁하나다', '.', '▁인권', '▁문제와', '▁주한미', '군', '▁철수', '[MASK]', '▁한때', '[MASK]', '▁무죄', '▁불편', '하기도', '▁했다', '.', '▁1978', '년', '▁대한민국에', '▁대한', '▁북한의', '▁위협', '에', '▁대비해', '▁한미', '연합', '사를', '▁창설', '하면서', ',', '▁1982', '년까지', '▁3', '단', '계에', '▁걸쳐', '▁주한미', '군을', '▁철수', '하기로', '▁했다', '.', '▁그러나', '▁영업을', '▁β', '▁이주하였다', '▁법률', '▁정보', '기관', '·', '의', '회의', '▁반', '대에', '▁부딪', '혀', '▁주한미', '군은', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [4, 20, 21, 29, 30, 35, 38, 39, 43, 44, 45, 77, 79, 80, 112, 113, 114, 115], 'mask_label': ['▁미국', '▁로', '널드', '▁재', '선에', '▁말기에', '▁소련의', '▁아프가니스탄', '▁1980', '년', '▁하계', '▁문제로', '▁한미', '▁관계가', '▁주한미', '군', '사령', '부와']}\n",
      "enc_token: [5, 330, 37, 3290, 6, 18590, 42, 27892, 23937, 42, 27892, 11560, 22684, 1827, 1640, 27625, 663, 5249, 4460, 1547, 6, 6, 1169, 27803, 958, 113, 27596, 27944, 875, 6, 6, 9510, 27599, 276, 11034, 6, 870, 27713, 6, 6, 3232, 6322, 751, 6, 6, 6, 5825, 13948, 4398, 247, 3052, 28805, 27607, 5890, 1853, 27599, 4, 16415, 207, 4612, 410, 786, 704, 643, 1165, 1063, 55, 23859, 663, 35, 15550, 27599, 5636, 14964, 24438, 27722, 5337, 6, 3590, 6, 9544, 12019, 863, 345, 27599, 3331, 27625, 11525, 92, 9305, 3038, 27600, 25557, 12259, 2569, 451, 3574, 421, 27604, 2760, 673, 49, 27737, 1949, 1633, 24438, 1262, 5337, 2390, 345, 27599, 330, 15077, 22036, 21548, 2212, 1071, 1468, 27873, 27601, 511, 141, 867, 11574, 28178, 24438, 941, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0   243     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0   194  8631     0     0\n",
      "     0     0     0     0     0   174  2087     0     0     0     0 12145\n",
      "     0     0  5569  7676     0     0     0  1640 27625  2219     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0  4875     0 12259  4857     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0 24438 27722  3069  2576     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', ',', '▁박정희', '▁대통령이', '▁김재', '규', '▁중앙정보', '부', '장에', '▁의해', '▁살해', '된', '▁것에', '▁대해', '▁그는', '▁이', '▁사건으로', '▁큰', '[MASK]', '▁받았으며', ',', '▁사이', '러스', '▁밴', '스', '▁국무', '장', '관을', '▁조', '문사', '절로', '▁파견했다', '.', '▁공을', '▁파스칼', '▁관료이다', '▁군사', '▁반란', '과', '▁5.', '17', '▁쿠데타', '에', '[MASK]', '▁초기에는', '▁강하게', '[MASK]', '[MASK]', '[MASK]', '▁미국', '▁정부가', '▁신군', '부를', '▁설득', '하는데', ',', '▁한계가', '▁있었고', '▁결국', '▁묵', '인', '하는', '▁듯한', '▁태도를', '▁보이게', '▁됐다', '.', '[SEP]', '▁퇴임', '[MASK]', '▁민간', '▁자원을', '▁적극', '▁활용한', '▁비영리', '▁기구', '인', '▁카터', '▁재', '단을', '▁설립한', '▁뒤', '[MASK]', '▁실현', '을', '▁위해', '▁제', '▁3', '세계의', '▁선거', '▁감시', '▁활동', '[MASK]', '[MASK]', '▁벌', '레', '에', '▁의한', '▁드라', '쿤', '쿠르', '스', '▁질병', '[MASK]', '[MASK]', '▁위해', '▁힘썼다', '.', '▁미국의', '[MASK]', '[MASK]', '▁지원', '▁활동', ',', '▁사랑의', '▁집', '짓', '기', '▁운동', ',', '▁국제', '▁분쟁', '▁중재', '▁등의', '▁활동도', '▁했다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [18, 19, 20, 33, 34, 35, 43, 46, 47, 48, 69, 82, 92, 93, 103, 104, 109, 110], 'mask_label': ['▁충격을', '▁받았으며', ',', '▁12', '·', '12', '▁대해', '▁비난', '했으나', ',', '▁이후', '▁민주주의', '▁및', '▁기니', '▁방', '재를', '▁빈곤', '층']}\n",
      "enc_token: [5, 27604, 5298, 4864, 9918, 27958, 18525, 27638, 1312, 355, 2591, 27711, 2057, 433, 202, 8, 6322, 459, 6, 5325, 27604, 328, 2086, 1228, 27626, 4444, 27651, 1657, 53, 27181, 17544, 26520, 27599, 4250, 24032, 23825, 1250, 2342, 27644, 11262, 1695, 14078, 27600, 6, 6797, 7015, 6, 6, 6, 243, 3840, 26826, 1191, 5523, 1294, 27604, 20984, 2492, 875, 5374, 27628, 38, 10180, 11162, 16915, 3842, 27599, 4, 13826, 6, 3174, 17304, 2929, 20639, 16068, 6673, 27628, 25250, 174, 1574, 7301, 339, 6, 5031, 27607, 231, 30, 49, 21655, 822, 7049, 375, 6, 6, 813, 27740, 27600, 1332, 17378, 28956, 16453, 27626, 5225, 6, 6, 231, 19607, 27599, 679, 6, 6, 770, 375, 27604, 14003, 313, 28333, 27614, 887, 27604, 605, 4476, 13267, 507, 27328, 345, 27599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0 10688  5325 27604     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0   196 27873  1335\n",
      "     0     0     0     0     0     0     0   433     0     0  3560  1003\n",
      " 27604     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0   165     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0  9889     0\n",
      "     0     0     0     0     0     0     0     0   228 18137     0     0\n",
      "     0     0     0     0     0     0     0    95  4445     0     0     0\n",
      "     0 14197 28083     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁군사적', '▁활동에', '▁강한', '▁반대', '▁입장을', '▁보이고', '▁있다', '.', '▁특히', '▁국제', '▁분쟁', '▁조정을', '▁위해', '▁북한의', '[MASK]', '[MASK]', '▁아이', '티의', '▁세', '드', '라스', '[MASK]', '[MASK]', '▁팔', '레인', '스타', '인의', '▁하', '마스', ',', '▁보스', '니아의', '▁세르비아', '계', '▁정권', '▁같이', '[MASK]', '[MASK]', '[MASK]', '▁협상을', '▁거부', '하면서', '▁사태', '의', '▁위기를', '▁초래', '한', '▁인물', '▁및', '▁단체를', '▁직접', '▁만나', '▁분쟁', '의', '▁원인을', '▁근본적으로', '▁해결하기', '▁위해', '[MASK]', '[MASK]', '▁이', '▁과정에서', '▁미국', '▁행정', '부와', '▁갈등을', '[MASK]', '▁했지만', ',', '▁전직', '▁대통령의', '▁권한', '과', '▁재', '야', '[MASK]', '▁인사', '들의', '▁활약으로', '▁해결', '해', '▁나갔다', '.', '[SEP]', '▁1978', '년에', '▁채', '결', '된', '▁캠프', '데이', '비', '드', '▁협', '정의', '▁이', '행이', '▁지지', '부', '진', '▁하자', '▁중동', '▁분쟁', '[MASK]', '[MASK]', '▁해결하기', '▁위해', '▁1993', '년', '▁퇴임', '▁후', '▁직접', '▁이스라엘', '과', '▁팔', '레인', '스타', '인의', '▁오슬로', '▁협정을', '[MASK]', '▁내는', '▁데', '도', '▁성공했다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [15, 16, 19, 20, 21, 22, 23, 37, 38, 39, 40, 59, 60, 67, 76, 104, 105, 121], 'mask_label': ['▁김일성', ',', '▁세', '드', '라스', '▁장군', ',', '▁미국', '▁정부에', '▁대해', '▁협상을', '▁힘썼다', '.', '▁보이기도', '▁유명', '▁분', '제를', '▁이끌어']}\n",
      "enc_token: [5, 9641, 8253, 2632, 1216, 5168, 7010, 28, 27599, 698, 605, 4476, 23144, 231, 9305, 6, 6, 520, 10694, 74, 27681, 1951, 6, 6, 961, 5346, 936, 692, 27, 3678, 27604, 6076, 4174, 4543, 27704, 5752, 733, 6, 6, 6, 12149, 2324, 421, 4597, 27601, 11239, 8200, 27612, 1178, 228, 19762, 1069, 2142, 4476, 27601, 13635, 24806, 13425, 231, 6, 6, 8, 2208, 243, 895, 2576, 12627, 6, 3379, 27604, 5605, 5744, 3753, 27644, 174, 27775, 6, 3329, 247, 17462, 2317, 27645, 9946, 27599, 4, 3331, 169, 481, 27783, 27711, 10251, 4282, 27694, 27681, 617, 2028, 8, 7071, 1565, 27638, 27713, 3121, 8021, 4476, 6, 6, 13425, 231, 2062, 27625, 13826, 81, 1069, 3426, 27644, 961, 5346, 936, 692, 25213, 15990, 6, 7573, 189, 27627, 6608, 27599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0 11444 27604     0     0    74 27681  1951  4379 27604\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0   243  6633   433 12149     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0 19607\n",
      " 27599     0     0     0     0     0     0 23828     0     0     0     0\n",
      "     0     0     0     0   939     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0   147  1104     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0  7027     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁대한', '▁미국의', '▁군사적', '▁행동이', '▁임', '박', '했으나', ',', '▁미국', '▁전직', '▁대통령', '으로는', '▁처음으로', '▁북한', '을', '▁방문', '하고', '▁미국과', '▁북', '▁양국의', '▁중재', '에', '▁큰', '▁기여를', '▁해', '▁위기를', '▁해결', '했다는', '▁평가를', '▁받았다', '.', '▁또한', '▁이', '▁때', '▁김영삼', '▁대통령과', '▁김일성', '[MASK]', '[MASK]', '▁만남', '을', '▁주선', '했다', '.', '▁하지만', '▁그로부터', '▁수', '주일', '▁후', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁김일', '성과', '▁김영삼', '의', '▁정상회담', '은', '▁이루어지지', '▁못했다', '.', '[SEP]', '▁미국의', '▁관', '타나', '모', '▁수용', '소', '▁문제', ',', '[MASK]', '▁인권', '문제', '에서도', '▁관심이', '▁깊', '어', '▁유엔', '에', '▁유엔', '인권', '고등', '판', '무', '관의', '▁제도를', '▁시행', '하도록', '▁노력', '하여', '[MASK]', '[MASK]', '[MASK]', '▁유', '린', '에', '▁대해', '▁제', '약을', '▁하고', ',', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁만드는', '▁데', '▁기여', '하여', '▁독재', '자들', '▁함경', '▁인권', '유', '린', '범죄', '자를', '▁재판', '소로', '▁회', '부', '하여', '▁국제적인', '▁처벌을', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 39, 40, 41, 42, 51, 52, 53, 54, 73, 93, 94, 95, 104, 105, 106, 107, 114], 'mask_label': ['▁북한에', '▁주', '석의', '▁만남', '을', '▁김일', '성이', '▁갑자기', '▁사망하여', '▁세계의', '▁독재', '자들의', '▁인권', '▁국제', '형사', '재판', '소를', '▁같은']}\n",
      "enc_token: [5, 6, 92, 679, 9641, 18507, 273, 27914, 1003, 27604, 243, 5605, 663, 1030, 1307, 1876, 27607, 2017, 48, 5672, 251, 25764, 13267, 27600, 459, 13856, 87, 11239, 2317, 2351, 4549, 772, 27599, 276, 8, 84, 9133, 13799, 11444, 6, 6, 11842, 27607, 25754, 31, 27599, 589, 14313, 19, 10106, 81, 6, 6, 6, 6, 4636, 1693, 9133, 27601, 27509, 27613, 13475, 2041, 27599, 4, 679, 88, 8011, 27716, 2237, 27688, 550, 27604, 6, 5636, 5515, 643, 8181, 1910, 27633, 3708, 27600, 3708, 12972, 5059, 27841, 27725, 2429, 6520, 2404, 1816, 3375, 54, 6, 6, 6, 46, 27870, 27600, 433, 30, 3297, 644, 27604, 6, 6, 6, 6, 3002, 189, 2187, 54, 7559, 3989, 6185, 5636, 27690, 27870, 8822, 690, 2474, 2661, 270, 27638, 54, 12835, 19956, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0 25086     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0    37  5361 11842 27607     0     0     0     0     0\n",
      "     0     0     0  4636   684  5908 26809     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0  5467     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0  7559  2653  5636\n",
      "     0     0     0     0     0     0     0     0   605 17905  4731  1358\n",
      "     0     0     0     0     0     0   226     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39/767648317.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_39/767648317.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_39/767648317.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    }
   ],
   "source": [
    "# 라인 단위로 처리\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88840089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a4b8496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d540c8105d48049cd67f1d2964edfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39/2049745891.py:42: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_39/2049745891.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_39/2049745891.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 256000 256000\n"
     ]
    }
   ],
   "source": [
    "# 256000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=256000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137308d",
   "metadata": {},
   "source": [
    "#### 학습 시 메모리 4G만 씀 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901f4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4689380",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c181eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d955804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07db4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03c932ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c457239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c1a6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and linear\n",
    "        attn_out = tf.transpose(attn_out, [0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out, [batch_size, -1, self.d_model])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out)  # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48dea1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09a8569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74c110dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa17f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "405d7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a0d8310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 8,\n",
       " 'd_head': 32,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 12,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 32007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\n",
    "    \"d_model\": 256,\n",
    "    \"n_head\": 8, \n",
    "    \"d_head\": 32,  # d_model / n_head \n",
    "    \"dropout\": 0.1,\n",
    "    \"d_ff\": 1024,\n",
    "    \"layernorm_epsilon\": 0.001,\n",
    "    \"n_layer\": 12, \n",
    "    \"n_seq\": 256, \n",
    "    \"n_vocab\": 0, \n",
    "    \"i_pad\": 0 \n",
    "})\n",
    "\n",
    "config.n_vocab = len(vocab) \n",
    "config.i_pad = vocab.pad_id() \n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57f2c35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 34s 29ms/step - loss: 11.2070 - nsp_loss: 0.7707 - mlm_loss: 10.4363 - nsp_acc: 0.3000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 9.8725 - nsp_loss: 0.6784 - mlm_loss: 9.1941 - nsp_acc: 0.6000 - mlm_acc: 0.2900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7e2db6bb3fa0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 동작 확인\n",
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6587e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94703e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e43ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9097758b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "648d2c9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 17737472    enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 32007)  0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 17,803,776\n",
      "Trainable params: 17,803,776\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63db2310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 40000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94c613d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 2041s 508ms/step - loss: 21.5219 - nsp_loss: 0.6173 - mlm_loss: 20.9046 - nsp_acc: 0.6361 - mlm_lm_acc: 0.0951\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.09514, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 2037s 509ms/step - loss: 17.8396 - nsp_loss: 0.5812 - mlm_loss: 17.2585 - nsp_acc: 0.6750 - mlm_lm_acc: 0.1566\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.09514 to 0.15664, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 2039s 510ms/step - loss: 14.8282 - nsp_loss: 0.5699 - mlm_loss: 14.2583 - nsp_acc: 0.6953 - mlm_lm_acc: 0.2198\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.15664 to 0.21984, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 2039s 510ms/step - loss: 13.5201 - nsp_loss: 0.5591 - mlm_loss: 12.9611 - nsp_acc: 0.7153 - mlm_lm_acc: 0.2528\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.21984 to 0.25276, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 2039s 510ms/step - loss: 12.7341 - nsp_loss: 0.5452 - mlm_loss: 12.1889 - nsp_acc: 0.7385 - mlm_lm_acc: 0.2739\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.25276 to 0.27390, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 2040s 510ms/step - loss: 12.1262 - nsp_loss: 0.5270 - mlm_loss: 11.5992 - nsp_acc: 0.7649 - mlm_lm_acc: 0.2912\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.27390 to 0.29121, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 2040s 510ms/step - loss: 11.6199 - nsp_loss: 0.5049 - mlm_loss: 11.1150 - nsp_acc: 0.7939 - mlm_lm_acc: 0.3059\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.29121 to 0.30592, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 2039s 510ms/step - loss: 11.2131 - nsp_loss: 0.4816 - mlm_loss: 10.7314 - nsp_acc: 0.8224 - mlm_lm_acc: 0.3182\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.30592 to 0.31817, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 2040s 510ms/step - loss: 10.9299 - nsp_loss: 0.4645 - mlm_loss: 10.4655 - nsp_acc: 0.8425 - mlm_lm_acc: 0.3273\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.31817 to 0.32726, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 2040s 510ms/step - loss: 10.7803 - nsp_loss: 0.4551 - mlm_loss: 10.3251 - nsp_acc: 0.8527 - mlm_lm_acc: 0.3319\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.32726 to 0.33194, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Q. 모델을 학습시키고, 내용을 history에 담아주세요.\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(\n",
    "    pre_train_inputs,\n",
    "    pre_train_labels,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[save_weights],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd1e576e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427511a0",
   "metadata": {},
   "source": [
    "### 정성평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31da5ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델불러오기 \n",
    "pre_train_model.load_weights(f\"{model_dir}/bert_pre_train.hdf5\")\n",
    "\n",
    "# EX\n",
    "sentences = [\n",
    "    \"오바마는 미국의 [MASK] 선거에 출마했다.\",\n",
    "    \"계산 실력은 [MASK]적 사고력과 관련이 없다.\",\n",
    "    \"물 [MASK]는 하나의 산소와 두 개의 수소 [MASK]로 이루어져 있다.\"\n",
    "]\n",
    "\n",
    "# bert에 맞는토큰화 함수 \n",
    "def tokenize_sentences(sentences, vocab, max_len):\n",
    "    tokenized_data = []\n",
    "    for sentence in sentences:\n",
    "        tokens = [\"[CLS]\"] + vocab.encode_as_pieces(sentence) + [\"[SEP]\"]\n",
    "        segments = [0] * len(tokens)\n",
    "        # 패딩 처리\n",
    "        tokens += [\"[PAD]\"] * (max_len - len(tokens))\n",
    "        segments += [0] * (max_len - len(segments))\n",
    "        tokenized_data.append((tokens, segments))\n",
    "    return tokenized_data\n",
    "\n",
    "# MAX_LEN\n",
    "max_len = config.n_seq\n",
    "\n",
    "# 토큰화 \n",
    "tokenized_data = tokenize_sentences(sentences, vocab, max_len)\n",
    "\n",
    "results = []\n",
    "\n",
    "for tokens, segments in tokenized_data:\n",
    "    enc_tokens = [vocab.piece_to_id(token) for token in tokens]\n",
    "    segments = [0] * len(enc_tokens)\n",
    "    \n",
    "    # 배열 변환 \n",
    "    enc_tokens_array = np.array([enc_tokens])\n",
    "    segments_array = np.array([segments])\n",
    "    \n",
    "    # predict\n",
    "    logits_cls, logits_lm = pre_train_model.predict((enc_tokens_array, segments_array))\n",
    "    \n",
    "    # [MASK] indices\n",
    "    mask_indices = [idx for idx, token in enumerate(tokens) if token == \"[MASK]\"]\n",
    "    \n",
    "    # [MASK] token 예측 \n",
    "    predicted_tokens = []\n",
    "    for mask_idx in mask_indices:\n",
    "        predicted_token_id = int(np.argmax(logits_lm[0, mask_idx]))\n",
    "        predicted_token = vocab.id_to_piece(predicted_token_id)\n",
    "        predicted_tokens.append(predicted_token)\n",
    "    \n",
    "    generated_sentence = tokens.copy()\n",
    "    for mask_idx, predicted_token in zip(mask_indices, predicted_tokens):\n",
    "        generated_sentence[mask_idx] = predicted_token\n",
    "    \n",
    "    results.append({\n",
    "        \"original_sentence\": \"\".join(tokens).replace(\"[PAD]\", \"\"),\n",
    "        \"predicted_tokens\": predicted_tokens,\n",
    "        \"generated_sentence\": \"\".join(generated_sentence).replace(\"[PAD]\", \"\")\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1aa7e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: [CLS]▁오바마는▁미국의▁[MASK]▁선거에▁출마했다.[SEP]\n",
      "Predicted Tokens for [MASK]: ['▁대통령']\n",
      "Generated Sentence: [CLS]▁오바마는▁미국의▁▁대통령▁선거에▁출마했다.[SEP]\n",
      "--------------------------------------------------\n",
      "Original Sentence: [CLS]▁계산▁실력은▁[MASK]적▁사고력과▁관련이▁없다.[SEP]\n",
      "Predicted Tokens for [MASK]: ['▁의']\n",
      "Generated Sentence: [CLS]▁계산▁실력은▁▁의적▁사고력과▁관련이▁없다.[SEP]\n",
      "--------------------------------------------------\n",
      "Original Sentence: [CLS]▁물▁[MASK]는▁하나의▁산소와▁두▁개의▁수소▁[MASK]로▁이루어져▁있다.[SEP]\n",
      "Predicted Tokens for [MASK]: ['▁물', '▁']\n",
      "Generated Sentence: [CLS]▁물▁▁물는▁하나의▁산소와▁두▁개의▁수소▁▁로▁이루어져▁있다.[SEP]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for result in results:\n",
    "    print(\"Original Sentence:\", result[\"original_sentence\"])\n",
    "    print(\"Predicted Tokens for [MASK]:\", result[\"predicted_tokens\"])\n",
    "    print(\"Generated Sentence:\", result[\"generated_sentence\"])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f95b93",
   "metadata": {},
   "source": [
    "* bert 임베딩을 통해서 직접 분류나 다른 task에 집어넣어보고 싶었으나 시간이 부족했다. \n",
    "* 사실은 hugging face에서 받아온 kobert의 성능보다야 당연 저조할게 뻔하니 의욕도 부족했다. \n",
    "    * 차라리 hugging face에서 받아온 kobert의 구조나 pretrain에 사용된 데이터의 수준을 보는게 더 배울점이 있을 것 같다.\n",
    "* 문단 두 개를 이어붙여서 학습한다는 것과 [MASK]를 맞춘다는 개념으로 모델을 구축하다보니 생소한 점이 좀 있었던 것 같다. \n",
    "* 결과적으로 context의 정보를 얼마나 잘 포함시키느냐-가 관건\n",
    "* GPT를 사용하는 입장에서 이 부분에서 GPT를 능가하는 모델은 아직 없는 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
